<html lang = "en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
  <meta http-equiv="Content-Language" content="en_US" /> 
  <title>Algorithms & PDE</title>
  <meta name="author" content="Nestor Guillen">
  <link rel="stylesheet" type="text/css" href="main.css?v=1.1">
</head>

<script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        messageStyle: "none",
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
</script>

<body>
  
<!-- Header Starts Here  -->  

  <br>
   <br>
     
  <div id="title">
     
	 <div class="container">
		 	 
	   <!-- <img src="OldMain.jpg" width="716px" height="551px">-->
	   <img src="UT_TXST.png" width="716px" height="551px">

	   <div class="top-center"><h4><font color = white>Algorithms<br>&<br>PDE<br></font></h4><br><font color = white>Austin-San Marcos <br> May 13-19, 2024</font></div>

	   <div class="bottom-right"><font color = white>A workshop + summer school  <br> jointly held at UT + TXST </font></div>
	  	
				  
  </div>
  
  <br>
      <center>   
     |<a href = "#courses">School</a>
	 |<a href = "#workshop">Workshop</a> 
	 |<a href = "#schedule">Schedule</a> 
	 |<a href = "index.html">Registration (closed)</a>|
     </center>

<br>

<center><p> (Registration for the program is now closed, however, those interested are encouraged to reach out to the organizers, see below for contact information) </p></center>
	 
</div>

<div id="body">

<p>This is a week long join program held at the University of Texas and Texas State University with <a href="#NSF">support from the National Science Foundation</a>, it comprises a 4 day summer school and a research workshop. Topically, the week's events are centered at the intersection of PDE and algorithms and should be of interest to students and faculty with interests in scientific computing, analysis of PDE, optimization, computer graphics, and machine learning. The aim is to facilitate and increase collaborations across disciplines in mathematics, computer science, and engineering.  </p>


<p>The summer school (Monday through Thursday) will be held on the Texas State campus (in San Marcos), while the research workshop (Friday-Saturday-Sunday) will take place at the University of Texas campus (in Austin).</p> 

<a name="courses"></a>
<h1>Mini courses</h1>
<h2>(Held at Texas State Univerisity, May 13th-16th)</h2>
<br>

<table style="width:100%", >	

  <tr style="background-color:#ffffff;">
    <td><h2><a href="https://www.math.ucla.edu/~wgangbo/">Wilfrid Gangbo</a> <br>University of California Los Angeles</h2></td>
    <td><h2><a href="https://web.math.ucsb.edu/~majaco/">Matthew Jacobs</a> <br>University of California Santa Barbara</h2></td>

  </tr>

  <tr style="background-color:#ffffff;">

    <td><h2><a href="https://stepatrizi.altervista.org/">Stefania Patrizi</a> <br>University of Texas at Austin</h2></td>
    <td><h2><a href="https://cauribe.rice.edu/">Cesar Uribe</a> <br>Rice University</h2></td>	
  </tr>


</table>

<br>

<h1><a name= "workshop"> Workshop</a></h1>
<h2>(Held at The University of Texas at Austin, May 17th-19th)</h2>


<br>

<table style="width:100%", >	

  <tr style="background-color:#ffffff;">
    <td><h2><a href="https://people.umass.edu/jbirrell/">Jeremey Birrell</a> <br>UMass Amherst<br>& Texas State University</h2></td>
    <td><h2><a href="https://zaytam.github.io/">Matias Delgadino</a><br>University of Texas at Austin <br> &nbsp;</h2></td>	
   </tr>

  <tr style="background-color:#ffffff;">
    <td><h2><a href="https://nhatptnk8912.github.io/">Nhat Ho</a><br>University of Texas at Austin</h2></td>	  
    <td><h2><a href="https://web.math.ucsb.edu/~majaco/">Matthew Jacobs</a> <br>University of California Santa Barbara</h2></td>	
  </tr>

  <tr style="background-color:#ffffff;">
    <td><h2><a href="https://users.math.msu.edu/users/jun/">Jun Kitagawa</a> <br>Michigan State University</h2></td>	 
    <td><h2><a href="https://tarheels.live/cmoosm/">Caroline Moosm&uuml;ller</a> <br>University of North Carolina Chapel Hill</h2></td>
  </tr>

  <tr style="background-color:#ffffff;">	  
    <td><h2><a href="https://web.ma.utexas.edu/users/ytsai/">Richard Tsai </a> <br>University of Texas at Austin<br >& Oden Institute</h2></td>
	<td><h2><a href="https://cauribe.rice.edu/">Cesar Uribe</a> <br>Rice University <br>&nbsp;</h2></td>
  </tr>

  <tr style="background-color:#ffffff;">	  
    <td><h2><a href="https://neha-wadia.github.io/">Neha Wadia</a> <br>Flatiron Institute</h2></td>    
	<td><h2><a href="https://cse.umn.edu/math/li-wang">Li Wang</a> <br>University of Minnesota</h2></td>
  </tr>



</table>


<a name= "schedule"><h1>Schedule </h1></a>

<br>

<h1>School (May 13th-16th)</h1>

<br>
<h2>Location: <b>Texas State University campus</b> (San Marcos, TX)</h2> 

<h2>All lectures will take place at <a href="https://maps.app.goo.gl/2SBbxMu3RkZimc1s8">Ingram Hall</a> Room 3204.</h2>


<table style="width:100%">	

  <tr>
    <td><strong>Time</strong></td>
    <td><strong>Monday</strong></td>
    <td><strong>Tuesday</strong></td>
    <td><strong>Wednesday</strong></td>	
    <td><strong>Thursday</strong></td>		
  </tr>
  <tr>
    <td>09:00 am - 09:20 am</td>
    <td>Registration + setup</td>
    <td>Informal discussion</td>
    <td>Informal discussion</td>	
    <td>Informal discussion</td>
  </tr>
  <tr>
    <td>09:30 am - 10:30 am</td>
    <td><a href="#GuillenAbstract">Intro lecture (Guillen)</a></td>
    <td><a href="#GangboAbstract1">Gangbo Lecture 1</a></td>
    <td><a href="#GangboAbstract1">Gangbo Lecture 3</a></td>
    <td>Jacobs Lecture 3 </td>	
  </tr>
  <tr>
    <td>10:30 am - 10:50 am</td>
    <td>Break</td>
    <td>Break</td>
    <td>Break</td>
    <td>Break</td>	
  </tr>
  <tr>
    <td>11:00 am - 12:00 pm</td>
    <td><a href="#PatriziAbstract">Patrizi Lecture 1</a></td>
    <td><a href="#GangboAbstract1">Gangbo Lecture 2</a></td>
    <td><a href="#GangboAbstract2">Gangbo Lecture 4</a></td>
    <td><a href="#UribeAbstractSchool">Uribe Lecture 3</a></td>	
  </tr>  
  <tr>
    <td>12:00 pm - 01:30 pm</td>
    <td>Lunch break</td>
    <td>Lunch break</td>
    <td>Lunch break</td>
    <td>Lunch break</td>	
  </tr> 
  <tr>
    <td>01:30 pm - 02:30 pm</td>
    <td><a href="#PatriziAbstract">Patrizi Lecture 2</a></td>
    <td><a href="#UribeAbstractSchool">Uribe Lecture 1</a></td>
    <td>Jacobs Lecture 1</td>
    <td><a href="#PatriziAbstract">Patrizi Lecture 4</a></td>	
  </tr>      
  <tr>
    <td>02:40 pm - 03:00 pm</td>
    <td>Break</td>
    <td>Break</td>
    <td>Break</td>
    <td>Break</td>	
  </tr>        
  <tr>
    <td>03:00 pm - 04:00 pm</td>
    <td><a href="#PatriziAbstract">Patrizi Lecture 3</a></td>
    <td><a href="#UribeAbstractSchool">Uribe Lecture 2</td>
    <td>Jacobs Lecture 2</td>
    <td><a href="#UribeAbstractSchool">Uribe Lecture 4</a></td>	
  </tr>        
  <tr>
    <td>04:10 pm - 05:10 pm</td>
    <td>Informal discussion</td>
    <td>Informal discussion</td>
    <td>Informal discussion</td>
    <td>Jacobs Lecture 4</td>	
  </tr>      

</table>

<br>
<br>


<h1>Workshop (May 17th-19th)</h1> 

<br>

<h2>Location: <b>The University of Texas at Austin campus</b> (Austin, TX)</h2>

<h2>All talks will take place at <a href="https://maps.app.goo.gl/etHgP2vHCrvAZLjd9">PMA</a> Room 6.104.</h2>

<table style="width:100%">	

  <tr>
    <td><strong>Time</strong></td>
    <td><strong>Friday</strong></td>
	<td><strong>Time</strong></td>
    <td><strong>Saturday</strong></td>
	<td><strong>Time</strong></td>
    <td><strong>Sunday</strong></td>	
  </tr>

  <tr>
	<td>1:00 - 1:30 pm</td> 
	<td>Registration </td>
	<td></td>
	<td></td>
	<td></td>
	<td></td> 
  </tr>
  
  <tr>
	<td>1:30 - 2:30 pm</td> 
	<td><a href="#UribeAbstractWorkshop">Cesar Uribe</a></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td> 
  </tr>
  
  <tr>
	<td>3:00 - 4:00 pm</td> 
	<td>Matthew Jacobs</td>
	<td></td>
	<td></td>
	<td></td>
	<td></td> 
  </tr>
  
  <tr>
	<td>4:10 - 5:10 pm</td> 
	<td><a href="#DelgadinoAbstract">Matias Delgadino</a></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td> 
  </tr>  
  
  <tr>
	<td></td> 
	<td></td>
	<td>9:00 - 10:00 am</td>
	<td><a href="#HoAbstract">Nhat Ho</a></td>
	<td></td>
	<td></td> 
  </tr>
  
  
  <tr>
	<td></td> 
	<td></td>
	<td>10:30 - 11:30 am</td>
	<td>Neha Wadia</td>
	<td></td>
	<td></td> 
  </tr>
  
    
  <tr>
	<td></td> 
	<td></td>
	<td>1:30 - 2:30 pm</td>
	<td><a href="#KitagawaAbstract">Jun Kitagawa</a></td>
	<td></td>
	<td></td> 
  </tr>  
  
  <tr>
	<td></td> 
	<td></td>
	<td>3:00 - 4:00 pm</td>
	<td><a href="#MoosmuellerAbstract">Caroline Moosm&uuml;ller</a></td>
	<td></td>
	<td></td> 
  </tr>   

  <tr>
	<td></td> 
	<td></td>
	<td>4:15 - 5:15 pm</td>
	<td><a href="#BirrellAbstract">Jeremey Birrell</a></td>
	<td></td>
	<td></td> 
  </tr>   

  
  
  <tr>
	<td></td> 
	<td></td>
	<td></td>
	<td></td>
	<td>9:00 - 10:00 am</td>
	<td><a href="#WangAbstract">Li Wang</a></td> 
  </tr>  
  
  <tr>
	<td></td> 
	<td></td>
	<td></td>
	<td></td>
	<td>10:30 - 11:30 am</td>
	<td><a href="#TsaiAbstract">Richard Tsai</a></td> 
  </tr>  
	  
</table>	  

<br>


<h1> Organizers </h1>

<br>

<h2><a href="https://web.ma.utexas.edu/users/gualdani/">Maria Gualdani</a> (UT Austin)</h2>

<h2><a href="https://www.ndguillen.com">Nestor Guillen</a> (TXST)</h2>

<p>For more information, email gualdani@math.utexas.edu and nestor@txstate.edu.</p>


<center>
<img src="NSF_logo.png">
</center>

<center>
<a name="NSF"> </a> We gratefully acknowledge support from the National Science Foundation through CAREER Grant <a href ="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2144232">DMS-2144232</a>, CAREER Grant <a href ="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2019335">DMS-2019335</a>, and UT Austin's RTG (Analysis of PDE) Grant <a href ="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1840314">DMS-1840314</a>.
</center>





</div>
<br>
<br>
<br>


</body>


<div id="GuillenAbstract" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Algorithms and PDE, a survey of (relatively) recent developments</h2>
		<h2>Nestor Guillen, Texas State University</h2>
		
		<h2>Times: Monday 9:30 - 10:30 am.</h2>
		<h2>Location: (Texas State) <a href="https://maps.app.goo.gl/2SBbxMu3RkZimc1s8">Ingram Hall</a> Room 3204. </h2>
		<div class="content">
		
		<p>As an introduction to the school, I will give an elementary introduction to some exciting topics related to algorithms and PDE of <i>relatively recent</i> origin (some as far back as the 1980's) and some from the current decade. These include the development, on one hand, of mathematical theories for Hamilton-Jacobi equations, optimal transport, and gradient flows, and on the other, of novel algorithms and computational frameworks like ADMM, stochastic gradient descent, and diffusion models. After a superficial survey of these topics I will discuss some broad analytical and computational challenges and how they might relate to one another. My hope is this might serve as motivation for the topics of the week's lectures and research talks, and point to the untapped potential collaboration across these various fields.</p>

		</div>
	</div>
</div>

<div id="PatriziAbstract" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>An introduction to the theory of viscosity solutions</h2>
		<h2>Stefania Patrizi, University of Texas at Austin</h2>
		
		<h2>Times: Monday 11 am - 12 pm, 1:30 pm-2:30 pm, 3 - 4 pm, and Thursday 1:30 pm - 2:30 pm.</h2>
		<h2>Location: (Texas State) <a href="https://maps.app.goo.gl/2SBbxMu3RkZimc1s8">Ingram Hall</a> Room 3204. </h2>
		<div class="content">
		
		<p>The notion of viscosity solution was introduced in 1983 by M. G. Crandall and  P. L. Lions as a generalization of the classical concept of solution for certain first order partial differential equations called Hamilton-Jacobi equations. This mini-course is an introduction to the theory of viscosity solutions later developed for degenerate elliptic and parabolic equations. In the first lecture I will define the notion of viscosity (sub,super-)solution and describe the class of equations treated within the theory. In the second lecture I will present the comparison principle between viscosity sub and super-solutions. Existence through the Perron's method will be then discussed in Lecture 3. Finally, Lecture 4 will be  devoted to an overview of the regularity theory.</p>

		</div>
	</div>
</div>

<div id="GangboAbstract1" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Lectures 1, 2, and 3: An introduction to the direct methods of the calculus of variations</h2>
		<h2>Wilfrid Gangbo, University of California Los Angeles</h2>
		
		<h2>Times: Tuesday 9:30-10:30 am, 11 am-12 pm, and Wednesday 9:30-10:30 am, 11 am-12 pm.</h2>
		<h2>Location: (Texas State) <a href="https://maps.app.goo.gl/2SBbxMu3RkZimc1s8">Ingram Hall</a> Room 3204. </h2>
		<div class="content">
		
		<p>In the middle of the last century, the so-called direct methods of the calculus of variations which was developed by Charles Bradfield Morrey, aims to study multiple integral variational problems. Morrey introduced the concept of quasiconvexity and proved that it is the right notion to guaranty existence of solutions for a class of variational problems in PDEs. Since it is hard to check whether or not a function is quasiconvex, Morrey introduced a new concept, called rank-one-convexity which is weaker than quasiconvexity but easier to handle. He also introduced the concept of polyconvexity which is stronger than quasiconvexity but also easier to handle. The first three lectures serve as an introduction to multiple integral variational problems. As a preamble, we will provide a brief introduction to Sobolev functions, which are unavoidable in the study of problems arising in nonlinear elasticity theory.</p>

		</div>
	</div>
</div>		

<div id="GangboAbstract2" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Lecture 4: Can computational math help settle down Morrey's and Iwaniec's conjectures?</h2>		
		<h2>Wilfrid Gangbo, University of California Los Angeles</h2>
		
		<h2>Time: Wednesday 11 am-12 pm.</h2>
		<h2>Location: (Texas State) <a href="https://maps.app.goo.gl/2SBbxMu3RkZimc1s8">Ingram Hall</a> Room 3204. </h2>
		<div class="content">
		
		<p font=lucida>In 1987, D. L. Burkholder proposed a very simple looking and explicit energy functionals $I_p$ defined on $\mathbb{S}$, the set of smooth functions on the complex plane. A question of great interest is to know whether or not $\sup_{\mathbb{S}} I_p \leq 0$. Since, the function $I_p$ is homogeneous of degree $p$, it is very surprising that it remains a challenge to prove or disprove that $\sup_{\mathbb{S}} I_p \leq 0$. Would $\sup_{\mathbb{S}} I_p \leq 0$, the so-called Iwaniec's conjecture on the Beurling-Ahlfors Transform in harmonic analysis would hold. Would $\sup_{\mathbb{S}} I_p > 0$, the so-called Morrey's conjecture in elasticity theory would hold. Therefore proving or disproving that $\sup_{\mathbb{S}} I_p \leq 0$ is equally important. Since the computational capacity of computers has increased exponentially over the past decades, it is natural to hope that computational math could help settle down these two conjectures at once. </p>	
			
		</div>
	</div>
</div>		



<div id="UribeAbstractSchool" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>A quick journey on DISTRIBUTED LEARNING</h2>
		<h2>Cesar Uribe, Rice University</h2>
		
		<h2>Times: Tuesday 1:30-2:30 pm, 3-4 pm, and Thursday 11 am-12 pm, 3-4 pm.</h2>
		<h2>Location: (Texas State) <a href="https://maps.app.goo.gl/2SBbxMu3RkZimc1s8">Ingram Hall</a> Room 3204. </h2>
		<div class="content">
		
		<p font=lucida>Prerequisites: Basic linear algebra and probability theory. Graph theory. Control theory, optimization theory, signal processing, and machine learning knowledge is helpful.</p>
			
		<p font=lucida>Course Rationale: Intro 4-lecture mini-course on modern design and analysis methods for distributed and decentralized algorithms with applications to signal processing, optimization, control, and machine learning.</p>	
			
		<p font=lucida>Summary: This course will provide an introductory presentation of modern design and analysis methods for distributed and decentralized algorithms for signal processing, optimization, control, and machine learning applications. The course will focus on mathematical analysis techniques for the iteration, computational, and communication complexity of distributed data processing methods over networks, where data is generated, stored, or processed by groups of computational units or agents connected via communication channels over networks. The contents of this course lie in the intersection of network science, optimization, and machine learning.</p>

		<p>Contents</p>

		<p>Lecture 1: The (distributed) optimization problem</p>
		<p>Lecture 2: Consensus, gossip, and random walks</p>
		<p>Lecture 3: Distributed optimization algorithms</p>
		<p>Lecture 4: Applications: optimal transport, statistical estimation, opinion dynamics</p>


		</div>
	</div>
</div>		



	
<div id="UribeAbstractWorkshop" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>On Graphs with Finite-Time Consensus</h2>
		<h2>Cesar Uribe, Rice University</h2>
		<h2>Time and location: Friday 1:30-2:30 pm, at (UT Austin) <a href="https://maps.app.goo.gl/etHgP2vHCrvAZLjd9">PMA</a> Room 6.104. </h2>
		<div class="content">
		<p foint=lucida>In this talk, we present sequences of graphs satisfying the finite-time consensus property (i.e., iterating through such a finite sequence is equivalent to performing global or exact averaging) and their use in Gradient Tracking. We provide an explicit weight matrix representation of the studied sequences and prove its finite-time consensus property. Moreover, we incorporate the studied finite-time consensus topologies into Gradient Tracking and present a new algorithmic scheme called Gradient Tracking for Finite-Time Consensus Topologies (GT-FT). We analyze the new scheme for nonconvex problems with stochastic gradient estimates. Our analysis shows that the convergence rate of GT-FT does not depend on the heterogeneity of the agents' functions or the connectivity of any individual graph in the topology sequence. Furthermore, owing to the sparsity of the graphs, GT-FT requires lower communication costs than Gradient Tracking using the static counterpart of the topology sequence.</p>

		</div>
	</div>
</div>		


	
<div id="DelgadinoAbstract" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Generative Adversarial Networks: Dynamics</h2>
		<h2>Matias Delgadino, UT Austin</h2>
		<h2>Time and location: Friday 4:10-5:10 pm, at (UT Austin) <a href="https://maps.app.goo.gl/etHgP2vHCrvAZLjd9">PMA</a> Room 6.104. </h2>
		<div class="content">
		<p foint=lucida>Generative Adversarial Networks (GANs) was one of the first Machine Learning algorithms to be able to generate remarkably realistic synthetic images. In this presentation, we delve into the mechanics of the GAN algorithm and its profound relationship with optimal transport theory. Through a detailed exploration, we illuminate how GAN approximates a system of PDE, particularly evident in shallow network architectures. Furthermore, we investigate the phenomenon of mode collapse, a well-known pathological behavior in GANs, and elucidate its connection to the underlying PDE framework through an illustrative example.</p>

		</div>
	</div>
</div>	
	
<div id="HoAbstract" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Hierarchical and Sequential Perspectives on Sliced Wasserstein Distance</h2>
		<h2>Nhat Ho, UT Austin</h2>
		<h2>Time and location: Saturday 9:00-10:00 am, at (UT Austin) <a href="https://maps.app.goo.gl/etHgP2vHCrvAZLjd9">PMA</a> Room 6.104. </h2>
		<div class="content">
		<p foint=lucida> From its origins in work by Monge and Kantorovich, the Wasserstein distance has played an important role in the theory of mathematics. In the current era, the strong and increasing connection between optimization and machine learning has brought new applications of theWasserstein distance to the fore. In these applications, the focus is on learning the probability distributions underlying the Wasserstein distance formulation. However, the Wasserstein distance has been known to suffer from expensive computation and the curse of dimensionality. It creates several hurdles of using theWasser- stein distance in statistical machine learning applications. A well-known approach to overcome the statistical and computational limits of the Wasserstein distance is by projecting the probability distributions into the one-dimensional manifold, which refer to as the sliced Wasserstein distance. The sliced Wasserstein distance leverages the closed-form expression of the Wasserstein distance in one dimension; therefore, its computational complexity is only linear in the number of supports of the probability distributions while the statistical rate is parametric for learning probability distributions. Despite these advantages of the sliced Wasserstein distance, it still suffers from two fundamental challenges in large-scale high dimensional statistical machine learning settings: (1) High projection complexities, namely, the number of projections to approximate the value of the sliced Wasserstein distance is huge and scales with the dimension of the problem; (2) Uninformative projecting directions, namely, there are several redundant projections to approximate the value of the sliced Wasserstein distance.</p>
		
		<p>In this talk, we propose two fundamental approaches to tackle the above challenges of the sliced Wasserstein distance. Our first approach hierarchically projects probability measures into low dimensional spaces before projecting them to one-dimensional space. The hierarchical projections lead to an improvement in projection complexity and enhance the expressiveness of the projection of the sliced Wasserstein distance. Our second approach considers sequential sampling for projecting directions to allow the sharing of information on new projecting directions based on the previous directions. It increases the quality of projections in terms of highlighting the difference between the probability measures and leads to a smaller number of projections, which improves the computational complexity of the sliced Wasserstein distance.</p>

		</div>
	</div>
</div>		

<div id="KitagawaAbstract" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Disintegrated transport metrics on fiber bundles</h2>
		<h2>Jun Kitagawa, Michigan State University</h2>
		<h2>Time and location: Saturday 1:30-2:30 pm, at (UT Austin) <a href="https://maps.app.goo.gl/etHgP2vHCrvAZLjd9">PMA</a> Room 6.104. </h2>
		<div class="content">
		<p foint=lucida>Some recent results have shown that the so-called sliced Wasserstein distances have wildly different geometric properties from those of classical optimal transport metrics, as such they are often not suitable as direct replacements. In this talk I will discuss some of these differences, and then introduce the family of disintegrated Monge-Kantorovich metrics defined on general metric fiber bundles. These metrics may be a more suitable alternative to the sliced ones in certain settings, and are of mathematical interest in their own right, generalizing the linear optimal transport metric, and being related to measure differential equations. This talk is based on joint work with Asuka Takatsu (Tokyo Metropolitan University).</p>
		</div>
	</div>
</div>	
	
	
<div id="MoosmuellerAbstract" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Manifold learning in Wasserstein space</h2>
		<h2>Caroline Moosm&uuml;ller, UNC Chapel Hill</h2>
		<h2>Time and location: Saturday 3:00-4:00 pm, at (UT Austin) <a href="https://maps.app.goo.gl/etHgP2vHCrvAZLjd9">PMA</a> Room 6.104. </h2>
		<div class="content">
		<p foint=lucida>In this talk, we introduce LOT Wassmap, a computationally feasible algorithm to uncover low-dimensional structures in the Wasserstein space. The algorithm is motivated by the observation that many datasets are naturally interpreted as probability measures rather than points in $\mathbb{R}^n$, and that finding low-dimensional descriptions of such datasets requires manifold learning algorithms in the Wasserstein space. Most available algorithms are based on computing the pairwise Wasserstein distance matrix, which can be computationally challenging for large datasets in high dimensions. Our algorithm leverages approximation schemes such as Sinkhorn distances and linearized optimal transport to speed-up computations, and in particular, avoids computing a pairwise distance matrix. Experiments demonstrate that LOT Wassmap attains correct embeddings and that the quality improves with increased sample size. We also show how LOT Wassmap significantly reduces the computational cost when compared to algorithms that depend on pairwise distance computations.</p>

		</div>
	</div>
</div>	
	
<div id="BirrellAbstract" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Generative Modeling: Information-Theoretic, Optimal Transport, and Dynamical Formulations</h2>
		<h2>Jeremey Birrell, UMass Amherst & Texas State University</h2>
		<h2>Time and location: Saturday 4:15-5:15 pm, at (UT Austin) <a href="https://maps.app.goo.gl/etHgP2vHCrvAZLjd9">PMA</a> Room 6.104. </h2>
		<div class="content">
		<p foint=lucida>Generative modeling is a rapidly growing area of machine learning wherein a model is trained on samples from a probability distribution (i.e., data) so as to learn the ability to produce new (approximate) samples from the distribution.  The mathematical techniques that underlie many successful approaches can be broadly categorized under information theoretic, optimal transport, and dynamical methods. Many approaches use more than one of these tools, either explicitly or implicitly.  In this talk I will introduce these different mathematical tools and discuss their use in several different approaches to training generative models.</p>

		</div>
	</div>
</div>	

<div id="WangAbstract" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Learning-enhanced structure preserving particle methods for nonlinear partial differential equations</h2>
		<h2>Li Wang, University of Minnesota</h2>
		<h2>Time and location: Sunday 9:00-10:00 am, at (UT Austin) <a href="https://maps.app.goo.gl/etHgP2vHCrvAZLjd9">PMA</a> Room 6.104. </h2>
		<div class="content">
		<p foint=lucida>In the current stage of numerical PDE, the primary challenge lies in addressing the complexities of high dimensionality while maintaining physical fidelity in our solvers. In this presentation, I will introduce deep learning assisted particle methods aimed at mitigating some of these challenges. Two scenarios will be considered, one is for general nonlinear Wasserstein-type gradient flow, and the other is for the Landau equation in plasma physics.</p>

		</div>
	</div>
</div>	

<div id="TsaiAbstract" class="overlay">
	<div class="popup">
		<a class="close" href="#!">&times;</a>

		<h2>Data-induced multiscale losses and efficient multirate gradient descent schemes</h2>
		<h2>Richard Tsai, UT Austin and Oden Institute</h2>
		<h2>Time and location: Sunday 10:30-11:30 am, at (UT Austin) <a href="https://maps.app.goo.gl/etHgP2vHCrvAZLjd9">PMA</a> Room 6.104. </h2>
		<div class="content">
		<p foint=lucida>We investigate the impact of multiscale data on machine learning algorithms, particularly in the context of deep learning. A dataset is multiscale if its distribution shows large variations in scale across different directions.  We show that the empirical loss will inherit certain multiscale properties from the data, even if the individual loss function does not have such properties. Correspondingly, we introduce a novel gradient descent approach, drawing inspiration from multiscale algorithms used in scientific computing. </p>
		</div>
	</div>
</div>	

</html>